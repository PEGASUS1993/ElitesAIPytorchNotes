{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 2020.2.14 第一次学习笔记"]},{"cell_type":"markdown","metadata":{},"source":["### 统计机器学习三要素：模型，策略，方法。"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3xxr5fb","id":"B116BFDF0D464FF49A85A582357D0B4D","jupyter":{},"mdEditEnable":true,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["# 1.线性回归\n","主要内容包括：\n","\n","1. 线性回归的基本要素\n","2. 线性回归模型从零开始的实现\n","3. 线性回归模型使用pytorch的简洁实现"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ht8ukap","id":"8FCA1BC77B7F479BA1398473C2691BB0","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 线性回归的基本要素\n","\n","### 模型\n","为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系:\n","\n","\n","$$\n","\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b\n","$$\n","\n","\n","\n","### 数据集\n","我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。\n","### 损失函数\n","在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估索引为 $i$ 的样本误差的表达式为\n","\n","\n","$$\n","l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,\n","$$\n","\n","\n","\n","$$\n","L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.\n","$$\n","\n","\n","### 优化函数 - 随机梯度下降\n","当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n","\n","在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\\mathcal{B}$，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。   \n","\n","$$\n","(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)\n","$$\n","  \n","学习率: $\\eta$代表在每次优化中，能够学习的步长的大小    \n","批量大小: $\\mathcal{B}$是小批量计算中的批量大小batch size   \n","\n","总结一下，优化函数的有以下两个步骤：\n","\n","- (i)初始化模型参数，一般来说使用随机初始化；\n","- (ii)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_v3gyr0b","id":"469D697FF90B48B7B0B61AED429EB8D6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 矢量计算\n","在模型训练或预测时，我们常常会同时处理多个数据样本并用到矢量计算。在介绍线性回归的矢量计算表达式之前，让我们先考虑对两个向量相加的两种方法。\n","\n","\n","1. 向量相加的一种方法是，将这两个向量按元素逐一做标量加法。\n","2. 向量相加的另一种方法是，将这两个向量直接做矢量加法。"]},{"cell_type":"markdown","metadata":{},"source":["后者比前者运算速度更快。因此，我们应该尽可能采用矢量计算，以提升计算效率。"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 2.多层感知机\n","1. 多层感知机的基本知识\n","2. 使用多层感知机图像分类的从零开始的实现\n","3. 使用pytorch的简洁实现"]},{"cell_type":"markdown","metadata":{},"source":["## 多层感知机的基本知识\n","深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。\n","\n","### 隐藏层\n","下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。\n","\n","![Image Name](https://cdn.kesci.com/upload/image/q5ho684jmh.png)\n","\n","### 表达公式\n","具体来说，给定一个小批量样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\\boldsymbol{H}$，有$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\\boldsymbol{W}_h \\in \\mathbb{R}^{d \\times h}$和 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，输出层的权重和偏差参数分别为$\\boldsymbol{W}_o \\in \\mathbb{R}^{h \\times q}$和$\\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times q}$。\n","\n","我们先来看一种含单隐藏层的多层感知机的设计。其输出$\\boldsymbol{O} \\in \\mathbb{R}^{n \\times q}$的计算为\n","\n","\n","$$\n"," \\begin{aligned} \\boldsymbol{H} &= \\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h,\\\\ \\boldsymbol{O} &= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o, \\end{aligned}\n","$$\n","\n","\n","也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到\n","\n","\n","$$\n"," \\boldsymbol{O} = (\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h)\\boldsymbol{W}_o + \\boldsymbol{b}_o = \\boldsymbol{X} \\boldsymbol{W}_h\\boldsymbol{W}_o + \\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o. \n","$$\n","\n","\n","从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\\boldsymbol{W}_h\\boldsymbol{W}_o$，偏差参数为$\\boldsymbol{b}_h \\boldsymbol{W}_o + \\boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。\n","\n","### 激活函数\n","上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。\n","\n","我们常用的几个激活函数：\n"]},{"cell_type":"markdown","metadata":{},"source":["#### ReLU函数\n","ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为\n","\n","\n","$$\n","\\text{ReLU}(x) = \\max(x, 0).\n","$$\n","\n","\n","可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。"]},{"cell_type":"markdown","metadata":{},"source":["#### Sigmoid函数\n","sigmoid函数可以将元素的值变换到0和1之间：\n","\n","\n","$$\n","\\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.\n","$$\n","\n","依据链式法则，sigmoid函数的导数\n","\n","\n","$$\n","\\text{sigmoid}'(x) = \\text{sigmoid}(x)\\left(1-\\text{sigmoid}(x)\\right).\n","$$\n","\n","\n","下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。\n","\n","#### tanh函数\n","tanh（双曲正切）函数可以将元素的值变换到-1和1之间：\n","\n","\n","$$\n","\\text{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.\n","$$\n","\n","\n","我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。\n"]},{"cell_type":"markdown","metadata":{},"source":["#### tanh函数\n","tanh（双曲正切）函数可以将元素的值变换到-1和1之间：\n","\n","\n","$$\n","\\text{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.\n","$$\n","\n","\n","我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。"]},{"cell_type":"markdown","metadata":{},"source":["### 关于激活函数的选择\n","\n","ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。\n","\n","用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  \n","\n","在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。\n","\n","在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。"]},{"cell_type":"markdown","metadata":{},"source":["### 多层感知机\n","多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：\n","\n","\n","$$\n"," \\begin{aligned} \\boldsymbol{H} &= \\phi(\\boldsymbol{X} \\boldsymbol{W}_h + \\boldsymbol{b}_h),\\\\ \\boldsymbol{O} &= \\boldsymbol{H} \\boldsymbol{W}_o + \\boldsymbol{b}_o, \\end{aligned} \n","$$\n","\n","\n","其中$\\phi$表示激活函数。"]},{"cell_type":"markdown","metadata":{},"source":["# 3.过拟合、欠拟合及其解决方案\n","1. 过拟合、欠拟合的概念\n","2. 权重衰减\n","3. 丢弃法"]},{"cell_type":"markdown","metadata":{},"source":["# 模型选择、过拟合和欠拟合\n","\n","## 训练误差和泛化误差\n","在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型在训练数据集上表现出的误差，后者指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。\n","\n","机器学习模型应关注降低泛化误差。\n","\n","## 模型选择\n","### 验证数据集\n","从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n","\n","### K折交叉验证  \n","由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n","## 过拟合和欠拟合\n","接下来，我们将探究模型训练中经常出现的两类典型问题：\n","- 一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n","- 另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n","在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。\n","\n","### 模型复杂度\n","为了解释模型复杂度，我们以多项式函数拟合为例。给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数\n","\n","\n","$$\n"," \\hat{y} = b + \\sum_{k=1}^K x^k w_k \n","$$\n","\n","\n","来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。\n","\n","给定训练数据集，模型复杂度和误差之间的关系：\n","\n","![Image Name](https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960)\n","\n","### 训练数据集大小\n","影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n"]},{"cell_type":"markdown","metadata":{},"source":["# 权重衰减\n","## 方法  \n","权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n","\n","##  L2 范数正则化（regularization）\n","$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例\n","\n","\n","$$\n"," \\ell(w_1, w_2, b) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right)^2 \n","$$\n","\n","\n","其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为\n","\n","\n","$$\n","\\ell(w_1, w_2, b) + \\frac{\\lambda}{2n} |\\boldsymbol{w}|^2,\n","$$\n","\n","\n","其中超参数$\\lambda > 0$。当权重参数均为0时，惩罚项最小。当$\\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。\n","有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为\n","\n","\n","$$\n"," \\begin{aligned} w_1 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\ w_2 &\\leftarrow \\left(1- \\frac{\\eta\\lambda}{|\\mathcal{B}|} \\right)w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right). \\end{aligned} \n","$$\n","\n","\n","可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n"]},{"cell_type":"markdown","metadata":{},"source":["# 丢弃法\n","\n","多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \\ldots, 5$）的计算表达式为\n","\n","\n","$$\n"," h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right) \n","$$\n","\n","\n","这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$\n","\n","\n","$$\n"," h_i' = \\frac{\\xi_i}{1-p} h_i \n","$$\n","\n","\n","由于$E(\\xi_i) = 1-p$，因此\n","\n","\n","$$\n"," E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i \n","$$\n","\n","\n","即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法\n","\n","![Image Name](https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 总结\n","\n","- 欠拟合现象：模型无法达到一个较低的误差\n","    \n","- 过拟合现象：训练误差较低但是泛化误差依然较高，二者相差较大\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4.梯度消失、梯度爆炸以及Kaggle房价预测\n","\n","1. 梯度消失和梯度爆炸\n","2. 考虑到环境因素的其他问题\n","3. Kaggle房价预测"]},{"cell_type":"markdown","metadata":{},"source":["# 梯度消失和梯度爆炸\n","\n","深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n","\n","**当神经网络的层数较多时，模型的数值稳定性容易变差。**\n","\n","假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n","\n","# 随机初始化模型参数\n","\n","在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。\n","\n","回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n","\n","\n","\n","![Image Name](https://cdn.kesci.com/upload/image/q5jg76kloy.png?imageView2/0/w/960/h/960)\n","\n","\n","\n","###  PyTorch的默认随机初始化\n","\n","随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑。\n","\n","\n","### Xavier随机初始化\n","\n","还有一种比较常用的随机初始化方法叫作Xavier随机初始化。\n","假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n","\n","\n","$$\n","U\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n","$$\n","\n","\n","它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}