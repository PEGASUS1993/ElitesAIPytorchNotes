{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二次学习笔记打卡2020.2.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_3dafypt",
    "id": "8C5B8FD488F74207A409DC379EAAFD63",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1.卷积神经网络基础\n",
    "\n",
    "本节我们介绍卷积神经网络的基础概念，主要是卷积层和池化层，并解释填充、步幅、输入通道和输出通道的含义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_hm819qn",
    "id": "254FA5A2E556460089490994E31604D0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 二维卷积层\n",
    "\n",
    "本节介绍的是最常见的二维卷积层，常用于处理图像数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1w29iz7",
    "id": "839259A7CAD349A7B2CE930E14F9E7B5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 二维互相关运算\n",
    "\n",
    "二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640)\n",
    "图1 二维互相关运算\n",
    "\n",
    "下面我们用`corr2d`函数实现二维互相关运算，它接受输入数组`X`与核数组`K`，并输出数组`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "graffitiCellId": "id_nculyfq",
    "id": "27AD46C637CE4BF88964BE3C22E6D4DB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "def corr2d(X, K):\n",
    "    H, W = X.shape\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros(H - h + 1, W - w + 1)\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_nochl8t",
    "id": "12E4EDD4780B4ED3BF46FC98E0671987",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "构造上图中的输入数组`X`、核数组`K`来验证二维互相关运算的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_ou9gykf",
    "id": "D6B5388CE30B4280A8EAC9C1E6160CED",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 25.],\n",
      "        [37., 43.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = torch.tensor([[0, 1], [2, 3]])\n",
    "Y = corr2d(X, K)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_kyrga1i",
    "id": "A667995033B54B998459E0CC127B6D96",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 二维卷积层\n",
    "\n",
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_wqfawao",
    "id": "02665E066AC949A2985642A92FC6352A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ywvx25d",
    "id": "C8662D63813F4E5DA12ABFE199E36F24",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "下面我们看一个例子，我们构造一张$6 \\times 8$的图像，中间4列为黑（0），其余为白（1），希望检测到颜色边缘。我们的标签是一个$6 \\times 7$的二维数组，第2列是1（从1到0的边缘），第6列是-1（从0到1的边缘）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_w4j38om",
    "id": "F7CC03E43D714175ADD7D490EE3BC0CB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 1.]])\n",
      "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones(6, 8)\n",
    "Y = torch.zeros(6, 7)\n",
    "X[:, 2: 6] = 0\n",
    "Y[:, 1] = 1\n",
    "Y[:, 5] = -1\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_re55xx5",
    "id": "03689A0772F94ABBBE066101211F6299",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们希望学习一个$1 \\times 2$卷积层，通过卷积层来检测颜色边缘。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "graffitiCellId": "id_d05ejpd",
    "id": "1D790E2D3B7E4835994C7198A830DA9D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5, loss 4.569\n",
      "Step 10, loss 0.949\n",
      "Step 15, loss 0.228\n",
      "Step 20, loss 0.060\n",
      "Step 25, loss 0.016\n",
      "Step 30, loss 0.004\n",
      "tensor([[ 1.0161, -1.0177]])\n",
      "tensor([0.0009])\n"
     ]
    }
   ],
   "source": [
    "conv2d = Conv2D(kernel_size=(1, 2))\n",
    "step = 30\n",
    "lr = 0.01\n",
    "for i in range(step):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = ((Y_hat - Y) ** 2).sum()\n",
    "    l.backward()\n",
    "    # 梯度下降\n",
    "    conv2d.weight.data -= lr * conv2d.weight.grad\n",
    "    conv2d.bias.data -= lr * conv2d.bias.grad\n",
    "    \n",
    "    # 梯度清零\n",
    "    conv2d.weight.grad.zero_()\n",
    "    conv2d.bias.grad.zero_()\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print('Step %d, loss %.3f' % (i + 1, l.item()))\n",
    "        \n",
    "print(conv2d.weight.data)\n",
    "print(conv2d.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_oytmkp4",
    "id": "6D2014DD040C4AE8928E8D2E8B0E2B27",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 互相关运算与卷积运算\n",
    "\n",
    "卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_iv5rdyk",
    "id": "232D9ADECF97429A802C60C0000EF53B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 特征图与感受野\n",
    "\n",
    "二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。\n",
    "\n",
    "以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \\times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_adfhv52",
    "id": "A4CE7D97E9744A138790A9700063083C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 填充和步幅\n",
    "\n",
    "我们介绍卷积层的两个超参数，即填充和步幅，它们可以对给定形状的输入和卷积核改变输出形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_oquq26z",
    "id": "6199F060EBF74BB9894EEDE4F3F845B5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 填充\n",
    "\n",
    "填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfl6ejy4.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "图2 在输入的高和宽两侧分别填充了0元素的二维互相关计算\n",
    "\n",
    "如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "(n_h+p_h-k_h+1)\\times(n_w+p_w-k_w+1)\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "我们在卷积神经网络中使用奇数高宽的核，比如$3 \\times 3$，$5 \\times 5$的卷积核，对于高度（或宽度）为大小为$2 k + 1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_co3p9ym",
    "id": "C31F0DD02A144FC29F738F6A1389E372",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 步幅\n",
    "\n",
    "在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，图3展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nflohnqg.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "图3 高和宽上步幅分别为3和2的二维互相关运算\n",
    "\n",
    "一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\\lfloor(n_h+p_h-k_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w+p_w-k_w+s_w)/s_w\\rfloor\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \\times (n_w/s_w)$。\n",
    "\n",
    "当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_1idmra0",
    "id": "D6F0F17D97B54D4E843A8792A8B2D568",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 多输入通道和多输出通道\n",
    "\n",
    "之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \\times h \\times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_n6snjjh",
    "id": "9D38698675574834BCF819E18FAA6F21",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 多输入通道\n",
    "\n",
    "卷积层的输入可以包含多个通道，图4展示了一个含2个输入通道的二维互相关计算的例子。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfmdnwbq.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "图4 含2个输入通道的互相关计算\n",
    "\n",
    "假设输入数据的通道数为$c_i$，卷积核形状为$k_h\\times k_w$，我们为每个输入通道各分配一个形状为$k_h\\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\\times k_h\\times k_w$的卷积核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_xpcj8zq",
    "id": "B43CBA5D4D854AB68291F5D7CB1B5FA5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 多输出通道\n",
    "\n",
    "卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。\n",
    "\n",
    "对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \\times k_h \\times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \\times k_h \\times k_w$的核数组，不同的核数组提取的是不同的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_t861gfe",
    "id": "2EDA6611AFD148D291D2B9E7AB8C1B50",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1x1卷积层\n",
    "\n",
    "最后讨论形状为$1 \\times 1$的卷积核，我们通常称这样的卷积运算为$1 \\times 1$卷积，称包含这种卷积核的卷积层为$1 \\times 1$卷积层。图5展示了使用输入通道数为3、输出通道数为2的$1\\times 1$卷积核的互相关计算。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfmq980r.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "图5 1x1卷积核的互相关计算。输入和输出具有相同的高和宽\n",
    "\n",
    "$1 \\times 1$卷积核可在不改变高宽的情况下，调整通道数。$1 \\times 1$卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么$1\\times 1$卷积层的作用与全连接层等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_6hb1wnk",
    "id": "DFB80D1A07A5496194BA81030D54E45E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 卷积层与全连接层的对比\n",
    "\n",
    "二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：\n",
    "\n",
    "一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。\n",
    "\n",
    "二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \\times c_o \\times h \\times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \\times c_2 \\times h_1 \\times w_1 \\times h_2 \\times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_h4crb33",
    "id": "C18215EE5FBC43F5867D1F384A2FC328",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 池化\n",
    "\n",
    "### 二维池化层\n",
    "\n",
    "池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。图6展示了池化窗口形状为$2\\times 2$的最大池化。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5nfob3odo.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "图6 池化窗口形状为 2 x 2 的最大池化\n",
    "\n",
    "二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \\times q$的池化层称为$p \\times q$池化层，其中的池化运算叫作$p \\times q$池化。\n",
    "\n",
    "池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。\n",
    "\n",
    "在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## course content\n",
    "\n",
    "1. lenet 模型介绍\n",
    "2. lenet 网络搭建\n",
    "3. 运用lenet进行图像识别-fashion-mnist数据集\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Networks\n",
    "\n",
    "使用全连接层的局限性：\n",
    "\n",
    "- 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。\n",
    "- 对于大尺寸的输入图像，使用全连接层容易导致模型过大。\n",
    "\n",
    "使用卷积层的优势：\n",
    "\n",
    "- 卷积层保留输入形状。\n",
    "- 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n",
    "\n",
    "## LeNet 模型\n",
    "\n",
    "LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ndwsmsao.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。\n",
    "\n",
    "卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。\n",
    "\n",
    "全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络就是含卷积层的网络。\n",
    "LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.# Transformer\n",
    "\n",
    "在之前的章节中，我们已经介绍了主流的神经网络架构如卷积神经网络（CNNs）和循环神经网络（RNNs）。让我们进行一些回顾：\n",
    "\n",
    "- CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。\n",
    "- RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。\n",
    "\n",
    "为了整合CNN和RNN的优势，[\\[Vaswani et al., 2017\\]](https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017) 创新性地使用注意力机制设计了Transformer模型。该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。\n",
    "\n",
    "图10.3.1展示了Transformer模型的架构，与9.7节的seq2seq模型相似，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：\n",
    "1. Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。\n",
    "2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。\n",
    "3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。\n",
    "\n",
    "![Fig. 10.3.1 The Transformer architecture.](https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "$$\n",
    "Fig.10.3.1\\ Transformer 架构.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力层\n",
    "\n",
    "在我们讨论多头注意力层之前，先来迅速理解以下自注意力（self-attention）的结构。自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。如图10.3.2 自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。\n",
    "\n",
    "![Fig. 10.3.2 自注意力结构](https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320)\n",
    "\n",
    "$$\n",
    "Fig.10.3.2\\ 自注意力结构\n",
    "$$\n",
    "\n",
    "\n",
    "多头注意力层包含$h$个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这$h$个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "$$\n",
    "Fig.10.3.3\\ 多头注意力\n",
    "$$\n",
    "\n",
    "\n",
    "假设query，key和value的维度分别是$d_q$、$d_k$和$d_v$。那么对于每一个头$i=1,\\ldots,h$，我们可以训练相应的模型权重$W_q^{(i)} \\in \\mathbb{R}^{p_q\\times d_q}$、$W_k^{(i)} \\in \\mathbb{R}^{p_k\\times d_k}$和$W_v^{(i)} \\in \\mathbb{R}^{p_v\\times d_v}$，以得到每个头的输出：\n",
    "\n",
    "\n",
    "$$\n",
    "o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v)\n",
    "$$\n",
    "\n",
    "\n",
    "这里的attention可以是任意的attention function，比如前一节介绍的dot-product attention以及MLP attention。之后我们将所有head对应的输出拼接起来，送入最后一个线性层进行整合，这个层的权重可以表示为$W_o\\in \\mathbb{R}^{d_0 \\times hp_v}$\n",
    "\n",
    "\n",
    "$$\n",
    "o = W_o[o^{(1)}, \\ldots, o^{(h)}]\n",
    "$$\n",
    "\n",
    "\n",
    "假设我们有h个头，隐藏层权重 $hidden\\_size = p_q = p_k = p_v$ 与query，key，value的维度一致。除此之外，因为多头注意力层保持输入与输出张量的维度不变，所以输出feature的维度也设置为 $d_0 = hidden\\_size$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。\n",
    "\n",
    "假设输入序列的嵌入表示 $X\\in \\mathbb{R}^{l\\times d}$, 序列长度为$l$嵌入向量维度为$d$，则其位置编码为$P \\in \\mathbb{R}^{l\\times d}$ ，输出的向量就是二者相加 $X + P$。\n",
    "\n",
    "位置编码是一个二维的矩阵，i对应着序列中的顺序，j对应其embedding vector内部的维度索引。我们可以通过以下等式计算位置编码：\n",
    "\n",
    "$$\n",
    "P_{i,2j} = sin(i/10000^{2j/d})\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P_{i,2j+1} = cos(i/10000^{2j/d})\n",
    "$$\n",
    "\n",
    "$$\n",
    "for\\ i=0,\\ldots, l-1\\ and\\ j=0,\\ldots,\\lfloor (d-1)/2 \\rfloor\n",
    "$$\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640)\n",
    "\n",
    "$$\n",
    "Fig. 10.3.4\\ 位置编码\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器\n",
    "Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。\n",
    "\n",
    "仔细来讲，在第t个时间步，当前输入$x_t$是query，那么self attention接受了第t步以及前t-1步的所有输入$x_1,\\ldots, x_{t-1}$。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5kpefhcyg.png?imageView2/0/w/800/h/800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
